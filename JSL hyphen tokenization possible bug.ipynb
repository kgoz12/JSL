{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spark-nlp==2.5.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Spark NLP\")\\\n",
    ".master(\"local[4]\")\\\n",
    ".config(\"spark.driver.memory\",\"16G\")\\\n",
    ".config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    ".config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4\")\\\n",
    ".config(\"spark.kryoserializer.buffer.max\",\"1000M\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, MapType, FloatType\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.5'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rowkey|                text|\n",
      "+------+--------------------+\n",
      "|     1|A dish is also be...|\n",
      "|     2|To me it seems ab...|\n",
      "|     3|Many kilns have e...|\n",
      "|     4|Pieces which have...|\n",
      "|     5|When the pieces a...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a toy dataframe with text I'd like to tinker with\n",
    "\n",
    "df = spark.createDataFrame([['1', 'A dish is also best thrown on a bat. No cylinder is made. The first step is to centre a flat disc of large diameter. The fingers are used as before to press down in the middle and are then drawn towards the edge preceded by a ridge of clay which gradually increases the diameter at each trip.'],                            \n",
    "                            ['2', 'To me it seems absurd to present someone who is only a beginner with anything heavier to throw than one pound of clay. In fact I would tend to commence with even less. This is because the problem of controlling and centering increases rapidly as the size of the lump becomes heavier. Long, tedious attempts to master a big piece only lead to frustration, disappointment, and material which soon becomes too wet to manage anyway. In my opinion, a smaller wheel and less powerful motor is quite sufficient for most students and for many serious workers too.'],\n",
    "                            ['3', 'Many kilns have elements running along the bottom as well as the sides and sometimes in the door and back. Even so, pots on the top shelf can easily be far cooler than those placed lower down in the kiln, If one superimposes on this additional discrepancy the ones we have just examined, it is easy to visualise that within the same firing chamber quite startling variations in temperature can occur.'],\n",
    "                            ['4', 'Pieces which have been painted with slip can usually be picked up with reasonable safety, provided the hands are clean and free of dust, but any which have had pottery colours applied to the surface should be held at points away from the pigment or from the inside. If a colour is inadvertantly smudged the damage can aften be repaired; use a razor blade to scratch away the smear and then very carefully fill in again with the paint brush. Where a slipped surface is chipped or otherwise marked the piece at this stage will be too dry to correct with slip, and one can either try and make good straightaway with the nearest available pottery colour or wait until after the biscuit firing and use colour before dipping the piece in glaze.'],\n",
    "                            ['5', 'When the pieces are being arranged, it is as well to remember that shelves should be as small as possible, consistent with their usefulness in supporting the ware. This is to avoid splitting the chamber into separate compartments. It is far better to allow the pots to overhang the edges a little and so permit the heat to circulate freely. A twelve inch square internal measurement will do best with a shelf no bigger than ten by ten.']],\n",
    "                           ['rowkey', 'text'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[summary: string, rowkey: string, text: string]\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "# view summary of data frame\n",
    "print(df.describe())\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document assember \n",
    "document_assembler = sparknlp.DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\\\n",
    ".setCleanupMode(\"shrink_full\")\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = sparknlp.annotator.Tokenizer()\\\n",
    ".setInputCols(\"document\")\\\n",
    ".setOutputCol(\"token\")\\\n",
    ".setTargetPattern(\"\\S+\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\))(\\\\.)\")\\\n",
    ".addInfixPattern(\"(/)(\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(\\\\p{Alpha}+)(/)(\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(\\\\p{Alpha}+)(\\\\.)(\\\\p{Upper}\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\.)\\\\z\")\\\n",
    ".addInfixPattern(\"(.+)([,:/])\\\\z\")\\\n",
    ".addInfixPattern(\"(\\\\()(.+)(\\\\))\")\\\n",
    ".addInfixPattern(\"(\\\\()(.+)\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\))\")\\\n",
    ".addException(\"New York\")\n",
    "# .addInfixPattern(\"(\\\\d+)(-)(\\\\d+)\")\\ # add this to top of infix pattern list for solution\n",
    "\n",
    "\n",
    "# context dependent spell checker\n",
    "spell_checker = sparknlp.annotator.ContextSpellCheckerApproach()\\\n",
    ".setInputCols([\"token\"])\\\n",
    ".setOutputCol(\"spell\")\\\n",
    ".setLanguageModelClasses(1000)\\\n",
    ".setWordMaxDistance(2)\\\n",
    ".setEpochs(2)\\\n",
    ".setMaxCandidates(10)\\\n",
    ".setClassCount(3)\\\n",
    ".setMaxWindowLen(15)\\\n",
    ".setTradeoff(10)#\\\n",
    "# .setMinCount(3)\n",
    "\n",
    "# pipeline\n",
    "pipeline =  Pipeline().setStages([document_assembler,\n",
    "                                          tokenizer, \n",
    "                                          spell_checker\n",
    "                                         ])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['Yesterday I saw 5 - 7 deer in my yard.'],\n",
       " 'token': ['Yesterday',\n",
       "  'I',\n",
       "  'saw',\n",
       "  '5',\n",
       "  '-',\n",
       "  '7',\n",
       "  'deer',\n",
       "  'in',\n",
       "  'my',\n",
       "  'yard',\n",
       "  '.'],\n",
       " 'spell': ['Yesterday',\n",
       "  '.',\n",
       "  'saw',\n",
       "  '.',\n",
       "  '-',\n",
       "  '.',\n",
       "  'deer',\n",
       "  'in',\n",
       "  'my',\n",
       "  'yard',\n",
       "  '.']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.annotate(\"Yesterday I saw 5 - 7 deer in my yard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'cannot create Tensors with a 0 dimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1734.annotateJava.\n: java.lang.IllegalArgumentException: cannot create Tensors with a 0 dimension\n\tat org.tensorflow.Tensor.fillShape(Tensor.java:750)\n\tat org.tensorflow.Tensor.fillShape(Tensor.java:759)\n\tat org.tensorflow.Tensor.create(Tensor.java:142)\n\tat org.tensorflow.Tensor.create(Tensor.java:126)\n\tat com.johnsnowlabs.ml.tensorflow.TensorResources.createTensor(TensorResources.scala:22)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowSpell$$anonfun$predict_$1.apply(TensorflowSpell.scala:92)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowSpell$$anonfun$predict_$1.apply(TensorflowSpell.scala:86)\n\tat scala.collection.immutable.List.flatMap(List.scala:338)\n\tat com.johnsnowlabs.ml.tensorflow.TensorflowSpell.predict_(TensorflowSpell.scala:86)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$decodeViterbi$2.apply$mcVI$sp(ContextSpellCheckerModel.scala:197)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$decodeViterbi$2.apply(ContextSpellCheckerModel.scala:182)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$decodeViterbi$2.apply(ContextSpellCheckerModel.scala:182)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.decodeViterbi(ContextSpellCheckerModel.scala:182)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$30$$anonfun$34.apply(ContextSpellCheckerModel.scala:315)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$30$$anonfun$34.apply(ContextSpellCheckerModel.scala:315)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$30.apply(ContextSpellCheckerModel.scala:315)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel$$anonfun$30.apply(ContextSpellCheckerModel.scala:306)\n\tat scala.collection.MapLike$MappedValues$$anonfun$iterator$3.apply(MapLike.scala:246)\n\tat scala.collection.MapLike$MappedValues$$anonfun$iterator$3.apply(MapLike.scala:246)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.MapLike$$anon$2.next(MapLike.scala:216)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)\n\tat scala.collection.generic.GenericTraversableTemplate$class.flatten(GenericTraversableTemplate.scala:171)\n\tat scala.collection.AbstractTraversable.flatten(Traversable.scala:104)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.annotate(ContextSpellCheckerModel.scala:326)\n\tat com.johnsnowlabs.nlp.LightPipeline$$anonfun$fullAnnotate$1.apply(LightPipeline.scala:32)\n\tat com.johnsnowlabs.nlp.LightPipeline$$anonfun$fullAnnotate$1.apply(LightPipeline.scala:20)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186)\n\tat com.johnsnowlabs.nlp.LightPipeline.fullAnnotate(LightPipeline.scala:20)\n\tat com.johnsnowlabs.nlp.LightPipeline.annotate(LightPipeline.scala:64)\n\tat com.johnsnowlabs.nlp.LightPipeline.annotateJava(LightPipeline.scala:80)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-8067681c9ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Yesterday I saw 5-7 deer in my yard.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sparknlp/base.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotateJava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'cannot create Tensors with a 0 dimension'"
     ]
    }
   ],
   "source": [
    "lp.annotate(\"Yesterday I saw 5-7 deer in my yard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
