{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spark-nlp==2.5.5\n",
    "# %pip install spylon-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Spark NLP\")\\\n",
    ".master(\"local[4]\")\\\n",
    ".config(\"spark.driver.memory\",\"16G\")\\\n",
    ".config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    ".config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5\")\\\n",
    ".config(\"spark.kryoserializer.buffer.max\",\"1000M\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import re\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, MapType, FloatType\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rowkey|                text|\n",
      "+------+--------------------+\n",
      "|     1|A dish is also be...|\n",
      "|     2|To me it seems ab...|\n",
      "|     3|Many kilns have e...|\n",
      "|     4|Pieces which have...|\n",
      "|     5|When the pieces a...|\n",
      "|     6|My 1 favorite pla...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a toy dataframe with text I'd like to tinker with\n",
    "\n",
    "df = spark.createDataFrame([['1', 'A dish is also best thrown on a bat. No cylinder is made. The first step is to centre a flat disc of large diameter. The fingers are used as before to press down in the middle and are then drawn towards the edge preceded by a ridge of clay which gradually increases the diameter at each trip.'],                            \n",
    "                            ['2', 'To me it seems absurd to present someone who is only a beginner with anything heavier to throw than one pound of clay. In fact I would tend to commence with even less. This is because the problem of controlling and centering increases rapidly as the size of the lump becomes heavier. Long, tedious attempts to master a big piece only lead to frustration, disappointment, and material which soon becomes too wet to manage anyway. In my opinion, a smaller wheel and less powerful motor is quite sufficient for most students and for many serious workers too.'],\n",
    "                            ['3', 'Many kilns have elements running along the bottom as well as the sides and sometimes in the door and back. Even so, pots on the top shelf can easily be far cooler than those placed lower down in the kiln, If one superimposes on this additional discrepancy the ones we have just examined, it is easy to visualise that within the same firing chamber quite startling variations in temperature can occur.'],\n",
    "                            ['4', 'Pieces which have been painted with slip can usually be picked up with reasonable safety, provided the hands are clean and free of dust, but any which have had pottery colours applied to the surface should be held at points away from the pigment or from the inside. If a colour is inadvertantly smudged the damage can aften be repaired; use a razor blade to scratch away the smear and then very carefully fill in again with the paint brush. Where a slipped surface is chipped or otherwise marked the piece at this stage will be too dry to correct with slip, and one can either try and make good straightaway with the nearest available pottery colour or wait until after the biscuit firing and use colour before dipping the piece in glaze.'],\n",
    "                            ['5', 'When the pieces are being arranged, it is as well to remember that shelves should be as small as possible, consistent with their usefulness in supporting the ware. This is to avoid splitting the chamber into separate compartments. It is far better to allow the pots to overhang the edges a little and so permit the heat to circulate freely. A twelve inch square internal measurement will do best with a shelf no bigger than ten by ten.'],\n",
    "                            ['6', 'My 1 favorite place is 2 blocks away. I drink 3 coffees and smell 4 flowers. Then I walk 5 dogs to 6 parks and see 7 friends. At 8 pm I head home and watch 9 movies.']],\n",
    "                           ['rowkey', 'text'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document assember \n",
    "document_assembler = sparknlp.DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\\\n",
    ".setCleanupMode(\"shrink_full\")\n",
    "\n",
    "# sentence detector\n",
    "sentence_detector = sparknlp.annotator.SentenceDetector()\\\n",
    ".setInputCols(\"document\")\\\n",
    ".setOutputCol(\"sentence\")\n",
    "\n",
    "# tokenizer \n",
    "tokenizer = sparknlp.annotator.Tokenizer()\\\n",
    ".setInputCols([\"sentence\"])\\\n",
    ".setOutputCol(\"token\")\\\n",
    ".setTargetPattern(\"\\S+\")\\\n",
    ".addInfixPattern(\"(\\\\d+)(-)(\\\\d+)\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\))(\\\\.)\")\\\n",
    ".addInfixPattern(\"(\\\\p{Alpha}+)(\\\\;)\")\\\n",
    ".addInfixPattern(\"(/)(\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(\\\\p{Alpha}+)(/)(\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(\\\\p{Alpha}+)(\\\\.)(\\\\p{Upper}\\\\p{Alpha}+)\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\.)\\\\z\")\\\n",
    ".addInfixPattern(\"(.+)([,:/])\\\\z\")\\\n",
    ".addInfixPattern(\"(\\\\()(.+)(\\\\))\")\\\n",
    ".addInfixPattern(\"(\\\\()(.+)\")\\\n",
    ".addInfixPattern(\"(.+)(\\\\))\")\\\n",
    ".addException(\"New York\")\n",
    "# .addInfixPattern(\"(\\\\d+)(-)(\\\\d+)\")\\ # add this to top of infix pattern list for solution\n",
    "\n",
    "# context dependent spell checker\n",
    "spell_checker = sparknlp.annotator.ContextSpellCheckerApproach()\\\n",
    ".setInputCols([\"token\"])\\\n",
    ".setOutputCol(\"spell\")\\\n",
    ".setLanguageModelClasses(1000)\\\n",
    ".setWordMaxDistance(2)\\\n",
    ".setEpochs(5)\\\n",
    ".setMaxCandidates(10)\\\n",
    ".setClassCount(3)\\\n",
    ".setMaxWindowLen(15)\\\n",
    ".setTradeoff(10)\n",
    "\n",
    "# finisher\n",
    "finisher = Finisher().setInputCols(\"spell\")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 spell_checker,\n",
    "                                 finisher\n",
    "                                ])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'spell': [Annotation(token, 61, 63, The, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 65, 67, are, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 69, 71, the, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 73, 77, costs, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 79, 83, fixed, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 85, 87, and, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 89, 91, not, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 93, 99, varying, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 101, 102, be, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 104, 109, token?, {'sentence': '2', 'cost': '372.18146042174266'}),\n",
       "   Annotation(token, 35, 37, The, {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 39, 45, weather, {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 47, 51, today, {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 53, 54, is, {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 56, 58, hot, {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 59, 59, ., {'sentence': '1', 'cost': '184.0707741911105'}),\n",
       "   Annotation(token, 0, 0, I, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 2, 5, have, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 7, 7, a, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 9, 15, meeting, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 17, 18, in, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 20, 20, a, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 22, 24, few, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 26, 32, minutes, {'sentence': '0', 'cost': '283.0195090866435'}),\n",
       "   Annotation(token, 33, 33, ., {'sentence': '0', 'cost': '283.0195090866435'})]}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think sentences need to be pipped into tokenizer; when you pipe in documents then the costs are equal for entire doc.\n",
    "lp.fullAnnotate(\"I have a meeting in a few minutes. The weather today is hot. Why are the costs fixed and not varying by token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spell': ['the',\n",
       "  'were',\n",
       "  '.',\n",
       "  '-',\n",
       "  '.',\n",
       "  'guests',\n",
       "  'at',\n",
       "  'the',\n",
       "  'party',\n",
       "  '.',\n",
       "  'be',\n",
       "  'met',\n",
       "  'on',\n",
       "  'January',\n",
       "  '1st',\n",
       "  'at',\n",
       "  'a',\n",
       "  'party',\n",
       "  ';']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.annotate(\"We met on January 1st at a party; there were 5-7 guests at the party.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rowkey|                text|\n",
      "+------+--------------------+\n",
      "|     1|We met on January...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNew = spark.createDataFrame([['1', 'We met on January 1st at a party; there were 5-7 guests at the party.']],\n",
    "                              ['rowkey', 'text'])\n",
    "dfNew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(rowkey='1', text='We met on January 1st at a party; there were 5-7 guests at the party.', finished_spell=['the', 'are', '.', '-', '.', 'guests', 'at', 'the', 'are', '.', 'be', 'be', 'on', 'January', 'is', 'at', 'a', 'party;'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.transform(dfNew)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't like \"5\" and \"7\" being replaced with commas by the spell checker...\n",
    "# spell_checker.addRegexClass('_NUM_', '[0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(sparknlp.annotator.ContextSpellCheckerApproach().addVocabClass)\n",
    "# dir(sparknlp.annotator.ContextSpellCheckerApproach)\n",
    "# sparknlp.annotator.ContextSpellCheckerApproach.getParamValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o6965.fit.\n: java.io.NotSerializableException: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1\nSerialization stack:\n\t- object not serializable (class: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1, value: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1@5bcc965f)\n\t- element of array (index: 2)\n\t- array (class [Lcom.johnsnowlabs.nlp.annotators.spell.context.parser.SpecialClassParser;, size 3)\n\t- field (class: scala.collection.mutable.WrappedArray$ofRef, name: array, type: class [Ljava.lang.Object;)\n\t- object (class scala.collection.mutable.WrappedArray$ofRef, WrappedArray(com.johnsnowlabs.nlp.annotators.spell.context.parser.DateToken$@581b9187, com.johnsnowlabs.nlp.annotators.spell.context.parser.NumberToken$@7e45150c, com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1@5bcc965f))\n\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:292)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)\n\tat com.johnsnowlabs.nlp.serialization.Feature$$anonfun$setValue$1.apply(Feature.scala:86)\n\tat com.johnsnowlabs.nlp.serialization.Feature$$anonfun$setValue$1.apply(Feature.scala:86)\n\tat scala.Option.map(Option.scala:146)\n\tat com.johnsnowlabs.nlp.serialization.Feature.setValue(Feature.scala:86)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.HasTransducerFeatures$class.set(HasTransducerFeatures.scala:12)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.set(ContextSpellCheckerModel.scala:17)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.setSpecialClassesTransducers(ContextSpellCheckerModel.scala:32)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach.train(ContextSpellCheckerApproach.scala:172)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach.train(ContextSpellCheckerApproach.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:55)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:61)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-780597cc41ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                    \u001b[0mfinisher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   ])\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# lp = LightPipeline(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# lp.annotate(\"We met on January 1st at a party; there were 5-7 guests at the party. 9 days later we took a trip.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6965.fit.\n: java.io.NotSerializableException: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1\nSerialization stack:\n\t- object not serializable (class: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1, value: com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1@5bcc965f)\n\t- element of array (index: 2)\n\t- array (class [Lcom.johnsnowlabs.nlp.annotators.spell.context.parser.SpecialClassParser;, size 3)\n\t- field (class: scala.collection.mutable.WrappedArray$ofRef, name: array, type: class [Ljava.lang.Object;)\n\t- object (class scala.collection.mutable.WrappedArray$ofRef, WrappedArray(com.johnsnowlabs.nlp.annotators.spell.context.parser.DateToken$@581b9187, com.johnsnowlabs.nlp.annotators.spell.context.parser.NumberToken$@7e45150c, com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach$$anon$1@5bcc965f))\n\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:292)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)\n\tat com.johnsnowlabs.nlp.serialization.Feature$$anonfun$setValue$1.apply(Feature.scala:86)\n\tat com.johnsnowlabs.nlp.serialization.Feature$$anonfun$setValue$1.apply(Feature.scala:86)\n\tat scala.Option.map(Option.scala:146)\n\tat com.johnsnowlabs.nlp.serialization.Feature.setValue(Feature.scala:86)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.HasTransducerFeatures$class.set(HasTransducerFeatures.scala:12)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.set(ContextSpellCheckerModel.scala:17)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel.setSpecialClassesTransducers(ContextSpellCheckerModel.scala:32)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach.train(ContextSpellCheckerApproach.scala:172)\n\tat com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach.train(ContextSpellCheckerApproach.scala:35)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:55)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:61)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# context dependent spell checker\n",
    "spell_checker1 = sparknlp.annotator.ContextSpellCheckerApproach()\\\n",
    ".setInputCols([\"token\"])\\\n",
    ".setOutputCol(\"spell\")\\\n",
    ".setLanguageModelClasses(1400)\\\n",
    ".addVocabClass(label = \"test\", vocab = ['blaa'], userdist = 3)\n",
    "\n",
    "# pipeline\n",
    "pipeline1 =  Pipeline().setStages([document_assembler,\n",
    "                                   sentence_detector,\n",
    "                                   tokenizer, \n",
    "                                   spell_checker1,\n",
    "                                   finisher\n",
    "                                  ])\n",
    "model = pipeline1.fit(df)\n",
    "# lp = LightPipeline(model)\n",
    "# lp.annotate(\"We met on January 1st at a party; there were 5-7 guests at the party. 9 days later we took a trip.\")\n",
    "\n",
    "\n",
    "# help(sparknlp.annotator.ContextSpellCheckerApproach().addRegexClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
